{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing ISI's COVID-19 Knowledge Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how we use our KGTK toolking (link) to construct the COVID-19 knowledge graph for the CORD-19 corpus using the text extractions from the BLENDER group at UIUC. The text extractions inlcude a variety of entity extractions with links to bioinformatics databases. Our approach is to:\n",
    "\n",
    "* extract from Wikidata the subgraph that covers all the publications in the CORD-19 corpus and the entities identified by the BLENDER group.\n",
    "* define new Wikidata items for publications or entities that are not present in the public Wikidata\n",
    "* annotate the article items with the relevant entities\n",
    "* preserve provenance\n",
    "\n",
    "We implement the approach in the following steps:\n",
    "\n",
    "* data prepararation: convert the JSON representation of BLENDER output to CSV files that are easy to process\n",
    "* extract Wikidata subgraph: extract from Wikidata the articles, authors, and entities mentioned in the BLENDER corpus\n",
    "* create missing items: create nodes for articles and entities that are not present in Wikidata\n",
    "* create mention edges: create edges to record the entity extractions from BLENDER, including justifications\n",
    "* incorporate analytic outputs: add edges to record graph metrics such as pagerank\n",
    "* export knowledge graph: export the graph to KGTL edges, RDF and Neo4J "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment variables with location of the input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COVID=/Users/pedroszekely/data/covid/blender\n",
      "env: WD=/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20200504\n"
     ]
    }
   ],
   "source": [
    "%env COVID=/Users/pedroszekely/data/covid/blender\n",
    "%env WD=/Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20200504"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikidata Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wikidata files are the large:\n",
    "\n",
    "* `wikidata_nodes_20200504.tsv.gz` the English labels, alias and descriptions for all items in Wikidata\n",
    "* `wikidata_edges_20200504.tsv.gz` the edges for all statements in Wikidata\n",
    "* `wikidata_qualifiers_20200504.tsv.gz` the qualifier edges for all statements in Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44833280\n",
      "-rw-------  1 pedroszekely  staff    21M May 11 14:19 P279.tsv.gz\n",
      "-rw-------  1 pedroszekely  staff   6.9M May 11 15:59 P279_star.tsv.gz\n",
      "-rw-------  1 pedroszekely  staff    12M May 11 14:29 P279_truncated.tsv.gz\n",
      "-rw-------  1 pedroszekely  staff   500M May 11 12:53 P31.tsv.gz\n",
      "-rw-------  1 pedroszekely  staff    16G May 11 04:09 wikidata_edges_20200504.tsv.gz\n",
      "-rw-------  1 pedroszekely  staff   2.2G May 11 03:22 wikidata_nodes_20200504.tsv.gz\n",
      "-rw-------  1 pedroszekely  staff   2.4G May 11 04:19 wikidata_qualifiers_20200504.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "ls -lh \"$WD\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with the dump files takes time because they are so large (86B chars and 1.1B lines), more than 6 minutes to just unzip and count lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1105944516 7412374276 86582473531\n",
      "\n",
      "real\t6m29.270s\n",
      "user\t8m28.469s\n",
      "sys\t1m17.519s\n"
     ]
    }
   ],
   "source": [
    "!time gzcat \"$WD/wikidata_edges_20200504.tsv.gz\" | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns in the edge files:\n",
    "\n",
    "* `id` is a unique identifier for an edge, and provides an identifier for each statement in Wikidata\n",
    "* `node1`, `label` and `node2` are the item/property/value or subject/predicate/object\n",
    "* `rank` is the Wikidata rank for the statement\n",
    "* `node2;*` are additional columns that provide detailed information about the `node2` column, making it easy to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\tnode1\tlabel\tnode2\trank\tnode2;magnitude\tnode2;unit\tnode2;date\tnode2;item\tnode2;lower\tnode2;upper\tnode2;latitude\tnode2;longitude\tnode2;precision\tnode2;calendar\tnode2;entity-type\n",
      "Q8-P1245-1\tQ8\tP1245\t\"885155\"\tnormal\t\t\t\t\t\t\t\t\t\t\t\n",
      "Q8-P373-1\tQ8\tP373\t\"Happiness\"\tnormal\t\t\t\t\t\t\t\t\t\t\t\n",
      "Q8-P31-1\tQ8\tP31\tQ331769\tnormal\t\t\t\tQ331769\t\t\t\t\t\t\titem\n",
      "Q8-P31-2\tQ8\tP31\tQ60539479\tnormal\t\t\t\tQ60539479\t\t\t\t\t\t\titem\n",
      "Q8-P31-3\tQ8\tP31\tQ9415\tnormal\t\t\t\tQ9415\t\t\t\t\t\t\titem\n",
      "Q8-P508-1\tQ8\tP508\t\"13163\"\tnormal\t\t\t\t\t\t\t\t\t\t\t\n",
      "Q8-P18-1\tQ8\tP18\t\"Sweet Baby Kisses Family Love.jpg\"\tnormal\t\t\t\t\t\t\t\t\t\t\t\n",
      "Q8-P910-1\tQ8\tP910\tQ8505256\tnormal\t\t\t\tQ8505256\t\t\t\t\t\t\titem\n",
      "Q8-P349-1\tQ8\tP349\t\"00566227\"\tnormal\t\t\t\t\t\t\t\t\t\t\t\n",
      "gzcat: error writing to output: Broken pipe\n",
      "gzcat: /Volumes/GoogleDrive/Shared drives/KGTK/datasets/wikidata-20200504/wikidata_edges_20200504.tsv.gz: uncompress failed\n"
     ]
    }
   ],
   "source": [
    "!gzcat \"$WD/wikidata_edges_20200504.tsv.gz\" | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLENDER Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a file that contains all the identifiers present in the BLENDER dataset with two columns:\n",
    "\n",
    "* node2: the value of the identifier\n",
    "* label: the name of the Wikidata property used to store the identifier, eg, P698 is PubMed ID\n",
    "\n",
    "Later, we are going to look up all these identifiers in Wikidata to find their q-nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node2\tlabel\n",
      "PMC3670673\tP932\n",
      "3670673\tP932\n",
      "22621853\tP698\n",
      "9606\tP685\n",
      "851819\tP351\n",
      "851323\tP351\n",
      "7905\tP351\n",
      "856140\tP351\n",
      "4932\tP685\n"
     ]
    }
   ],
   "source": [
    "!head $COVID/corpus-identifiers.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Wikidata Subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First extract all the edges that we may want to use (this takes 107 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "real\t107m13.861s\n",
      "user\t146m30.484s\n",
      "sys\t9m8.806s\n"
     ]
    }
   ],
   "source": [
    "!time gzcat \"$WD/wikidata_edges_20200504.tsv.gz\" | kgtk filter -p \";P685,P486,P351,P5055,P698,P932;\" | gzip > $COVID/corpus-identifier-edges.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Missing Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Mention Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporate Analytic Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!time gzcat $COVID/pmcid.tsv.gz | kgtk ifexists --filter-on $COVID/covid-pmcids.tsv --left-keys node2 --right-keys id | gzip > covid-pmcid-edges.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12934   64679  750209\n"
     ]
    }
   ],
   "source": [
    "!gzcat covid-pmcid-edges.tsv.gz | wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\tnode1\tlabel\tnode2\trank\tnode2;magnitude\tnode2;unit\tnode2;lower\tnode2;upper\tnode2;latitude\tnode2;longitude\tnode2;precision\tnode2;calendar\tnode2;entity-type\n",
      "Q21093209-P932-1\tQ21093209\tP932\t\"2918564\"\tnormal\t\t\t\t\t\t\t\t\t\n",
      "Q21142689-P932-1\tQ21142689\tP932\t\"2792043\"\tnormal\t\t\t\t\t\t\t\t\t\n",
      "Q21144693-P932-1\tQ21144693\tP932\t\"1564183\"\tnormal\t\t\t\t\t\t\t\t\t\n",
      "Q21146685-P932-1\tQ21146685\tP932\t\"4207625\"\tnormal\t\t\t\t\t\t\t\t\t\n",
      "Q21256658-P932-1\tQ21256658\tP932\t\"1373654\"\tnormal\t\t\t\t\t\t\t\t\t\n",
      "Q21283966-P932-1\tQ21283966\tP932\t\"1185527\"\tnormal\t\t\t\t\t\t\t\t\t\n",
      "Q21328696-P932-1\tQ21328696\tP932\t\"2842971\"\tnormal\t\t\t\t\t\t\t\t\t\n",
      "Q22680677-P932-1\tQ22680677\tP932\t\"4517126\"\tnormal\t\t\t\t\t\t\t\t\t\n",
      "Q23912860-P932-1\tQ23912860\tP932\t\"4941879\"\tnormal\t\t\t\t\t\t\t\t\t\n",
      "gzcat: error writing to output: Broken pipe\n",
      "gzcat: covid-pmcid-edges.tsv.gz: uncompress failed\n"
     ]
    }
   ],
   "source": [
    "!gzcat covid-pmcid-edges.tsv.gz | head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90mid\u001b[39m Q21093209\n",
      "\u001b[42mLabel\u001b[49m RETRACTED: Influenza or not influenza: analysis of a case of high fever that happened 2000 years ago in Biblical time\n",
      "\u001b[44mDescription\u001b[49m scientific article\n",
      "\u001b[30m\u001b[47minstance of\u001b[49m\u001b[39m \u001b[90m(P31)\u001b[39m\u001b[90m: \u001b[39mscholarly article \u001b[90m(Q13442814)\u001b[39m | retracted paper \u001b[90m(Q45182324)\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!wd u Q21093209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pedroszekely/data/covid/covid-pmcid-edges.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!ls $COVID/covid-pmcid-edges.tsv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: COVID: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cd $(env COVID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/pedroszekely/data/covid'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: mlr [I/O options] {verb} [verb-dependent options ...] {zero or more file names}\n",
      "\n",
      "Command-line-syntax examples:\n",
      "  mlr --csv cut -f hostname,uptime mydata.csv\n",
      "  mlr --tsv --rs lf filter '$status != \"down\" && $upsec >= 10000' *.tsv\n",
      "  mlr --nidx put '$sum = $7 < 0.0 ? 3.5 : $7 + 2.1*$8' *.dat\n",
      "  grep -v '^#' /etc/group | mlr --ifs : --nidx --opprint label group,pass,gid,member then sort -f group\n",
      "  mlr join -j account_id -f accounts.dat then group-by account_name balances.dat\n",
      "  mlr --json put '$attr = sub($attr, \"([0-9]+)_([0-9]+)_.*\", \"\\1:\\2\")' data/*.json\n",
      "  mlr stats1 -a min,mean,max,p10,p50,p90 -f flag,u,v data/*\n",
      "  mlr stats2 -a linreg-pca -f u,v -g shape data/*\n",
      "  mlr put -q '@sum[$a][$b] += $x; end {emit @sum, \"a\", \"b\"}' data/*\n",
      "  mlr --from estimates.tbl put '\n",
      "  for (k,v in $*) {\n",
      "    if (is_numeric(v) && k =~ \"^[t-z].*$\") {\n",
      "      $sum += v; $count += 1\n",
      "    }\n",
      "  }\n",
      "  $mean = $sum / $count # no assignment if count unset'\n",
      "  mlr --from infile.dat put -f analyze.mlr\n",
      "  mlr --from infile.dat put 'tee > \"./taps/data-\".$a.\"-\".$b, $*'\n",
      "  mlr --from infile.dat put 'tee | \"gzip > ./taps/data-\".$a.\"-\".$b.\".gz\", $*'\n",
      "  mlr --from infile.dat put -q '@v=$*; dump | \"jq .[]\"'\n",
      "  mlr --from infile.dat put  '(NR % 1000 == 0) { print > stderr, \"Checkpoint \".NR}'\n",
      "\n",
      "Data-format examples:\n",
      "  DKVP: delimited key-value pairs (Miller default format)\n",
      "  +---------------------+\n",
      "  | apple=1,bat=2,cog=3 | Record 1: \"apple\" => \"1\", \"bat\" => \"2\", \"cog\" => \"3\"\n",
      "  | dish=7,egg=8,flint  | Record 2: \"dish\" => \"7\", \"egg\" => \"8\", \"3\" => \"flint\"\n",
      "  +---------------------+\n",
      "\n",
      "  NIDX: implicitly numerically indexed (Unix-toolkit style)\n",
      "  +---------------------+\n",
      "  | the quick brown     | Record 1: \"1\" => \"the\", \"2\" => \"quick\", \"3\" => \"brown\"\n",
      "  | fox jumped          | Record 2: \"1\" => \"fox\", \"2\" => \"jumped\"\n",
      "  +---------------------+\n",
      "\n",
      "  CSV/CSV-lite: comma-separated values with separate header line\n",
      "  +---------------------+\n",
      "  | apple,bat,cog       |\n",
      "  | 1,2,3               | Record 1: \"apple => \"1\", \"bat\" => \"2\", \"cog\" => \"3\"\n",
      "  | 4,5,6               | Record 2: \"apple\" => \"4\", \"bat\" => \"5\", \"cog\" => \"6\"\n",
      "  +---------------------+\n",
      "\n",
      "  Tabular JSON: nested objects are supported, although arrays within them are not:\n",
      "  +---------------------+\n",
      "  | {                   |\n",
      "  |  \"apple\": 1,        | Record 1: \"apple\" => \"1\", \"bat\" => \"2\", \"cog\" => \"3\"\n",
      "  |  \"bat\": 2,          |\n",
      "  |  \"cog\": 3           |\n",
      "  | }                   |\n",
      "  | {                   |\n",
      "  |   \"dish\": {         | Record 2: \"dish:egg\" => \"7\", \"dish:flint\" => \"8\", \"garlic\" => \"\"\n",
      "  |     \"egg\": 7,       |\n",
      "  |     \"flint\": 8      |\n",
      "  |   },                |\n",
      "  |   \"garlic\": \"\"      |\n",
      "  | }                   |\n",
      "  +---------------------+\n",
      "\n",
      "  PPRINT: pretty-printed tabular\n",
      "  +---------------------+\n",
      "  | apple bat cog       |\n",
      "  | 1     2   3         | Record 1: \"apple => \"1\", \"bat\" => \"2\", \"cog\" => \"3\"\n",
      "  | 4     5   6         | Record 2: \"apple\" => \"4\", \"bat\" => \"5\", \"cog\" => \"6\"\n",
      "  +---------------------+\n",
      "\n",
      "  XTAB: pretty-printed transposed tabular\n",
      "  +---------------------+\n",
      "  | apple 1             | Record 1: \"apple\" => \"1\", \"bat\" => \"2\", \"cog\" => \"3\"\n",
      "  | bat   2             |\n",
      "  | cog   3             |\n",
      "  |                     |\n",
      "  | dish 7              | Record 2: \"dish\" => \"7\", \"egg\" => \"8\"\n",
      "  | egg  8              |\n",
      "  +---------------------+\n",
      "\n",
      "  Markdown tabular (supported for output only):\n",
      "  +-----------------------+\n",
      "  | | apple | bat | cog | |\n",
      "  | | ---   | --- | --- | |\n",
      "  | | 1     | 2   | 3   | | Record 1: \"apple => \"1\", \"bat\" => \"2\", \"cog\" => \"3\"\n",
      "  | | 4     | 5   | 6   | | Record 2: \"apple\" => \"4\", \"bat\" => \"5\", \"cog\" => \"6\"\n",
      "  +-----------------------+\n",
      "\n",
      "Help options:\n",
      "  -h or --help                 Show this message.\n",
      "  --version                    Show the software version.\n",
      "  {verb name} --help           Show verb-specific help.\n",
      "  --help-all-verbs             Show help on all verbs.\n",
      "  -l or --list-all-verbs       List only verb names.\n",
      "  -L                           List only verb names, one per line.\n",
      "  -f or --help-all-functions   Show help on all built-in functions.\n",
      "  -F                           Show a bare listing of built-in functions by name.\n",
      "  -k or --help-all-keywords    Show help on all keywords.\n",
      "  -K                           Show a bare listing of keywords by name.\n",
      "\n",
      "Verbs:\n",
      "   altkv bar bootstrap cat check clean-whitespace count-distinct count-similar\n",
      "   cut decimate fill-down filter format-values fraction grep group-by\n",
      "   group-like having-fields head histogram join label least-frequent\n",
      "   merge-fields most-frequent nest nothing put regularize remove-empty-columns\n",
      "   rename reorder repeat reshape sample sec2gmt sec2gmtdate seqgen shuffle\n",
      "   skip-trivial-records sort stats1 stats2 step tac tail tee top uniq\n",
      "   unsparsify\n",
      "\n",
      "Functions for the filter and put verbs:\n",
      "   + + - - * / // .+ .+ .- .- .* ./ .// % ** | ^ & ~ << >> bitcount == != =~\n",
      "   !=~ > >= < <= && || ^^ ! ? : . gsub regextract regextract_or_else strlen sub\n",
      "   ssub substr tolower toupper capitalize lstrip rstrip strip\n",
      "   collapse_whitespace clean_whitespace system abs acos acosh asin asinh atan\n",
      "   atan2 atanh cbrt ceil cos cosh erf erfc exp expm1 floor invqnorm log log10\n",
      "   log1p logifit madd max mexp min mmul msub pow qnorm round roundm sgn sin\n",
      "   sinh sqrt tan tanh urand urandrange urand32 urandint dhms2fsec dhms2sec\n",
      "   fsec2dhms fsec2hms gmt2sec localtime2sec hms2fsec hms2sec sec2dhms sec2gmt\n",
      "   sec2gmt sec2gmtdate sec2localtime sec2localtime sec2localdate sec2hms\n",
      "   strftime strftime_local strptime strptime_local systime is_absent is_bool\n",
      "   is_boolean is_empty is_empty_map is_float is_int is_map is_nonempty_map\n",
      "   is_not_empty is_not_map is_not_null is_null is_numeric is_present is_string\n",
      "   asserting_absent asserting_bool asserting_boolean asserting_empty\n",
      "   asserting_empty_map asserting_float asserting_int asserting_map\n",
      "   asserting_nonempty_map asserting_not_empty asserting_not_map\n",
      "   asserting_not_null asserting_null asserting_numeric asserting_present\n",
      "   asserting_string boolean float fmtnum hexfmt int string typeof depth haskey\n",
      "   joink joinkv joinv leafcount length mapdiff mapexcept mapselect mapsum\n",
      "   splitkv splitkvx splitnv splitnvx\n",
      "\n",
      "Please use \"mlr --help-function {function name}\" for function-specific help.\n",
      "\n",
      "Data-format options, for input, output, or both:\n",
      "  --idkvp   --odkvp   --dkvp      Delimited key-value pairs, e.g \"a=1,b=2\"\n",
      "                                  (this is Miller's default format).\n",
      "\n",
      "  --inidx   --onidx   --nidx      Implicitly-integer-indexed fields\n",
      "                                  (Unix-toolkit style).\n",
      "  -T                              Synonymous with \"--nidx --fs tab\".\n",
      "\n",
      "  --icsv    --ocsv    --csv       Comma-separated value (or tab-separated\n",
      "                                  with --fs tab, etc.)\n",
      "\n",
      "  --itsv    --otsv    --tsv       Keystroke-savers for \"--icsv --ifs tab\",\n",
      "                                  \"--ocsv --ofs tab\", \"--csv --fs tab\".\n",
      "  --iasv    --oasv    --asv       Similar but using ASCII FS 0x1f and RS 0x1e\n",
      "  --iusv    --ousv    --usv       Similar but using Unicode FS U+241F (UTF-8 0xe2909f)\n",
      "                                  and RS U+241E (UTF-8 0xe2909e)\n",
      "\n",
      "  --icsvlite --ocsvlite --csvlite Comma-separated value (or tab-separated\n",
      "                                  with --fs tab, etc.). The 'lite' CSV does not handle\n",
      "                                  RFC-CSV double-quoting rules; is slightly faster;\n",
      "                                  and handles heterogeneity in the input stream via\n",
      "                                  empty newline followed by new header line. See also\n",
      "                                  http://johnkerl.org/miller/doc/file-formats.html#CSV/TSV/etc.\n",
      "\n",
      "  --itsvlite --otsvlite --tsvlite Keystroke-savers for \"--icsvlite --ifs tab\",\n",
      "                                  \"--ocsvlite --ofs tab\", \"--csvlite --fs tab\".\n",
      "  -t                              Synonymous with --tsvlite.\n",
      "  --iasvlite --oasvlite --asvlite Similar to --itsvlite et al. but using ASCII FS 0x1f and RS 0x1e\n",
      "  --iusvlite --ousvlite --usvlite Similar to --itsvlite et al. but using Unicode FS U+241F (UTF-8 0xe2909f)\n",
      "                                  and RS U+241E (UTF-8 0xe2909e)\n",
      "\n",
      "  --ipprint --opprint --pprint    Pretty-printed tabular (produces no\n",
      "                                  output until all input is in).\n",
      "                      --right     Right-justifies all fields for PPRINT output.\n",
      "                      --barred    Prints a border around PPRINT output\n",
      "                                  (only available for output).\n",
      "\n",
      "            --omd                 Markdown-tabular (only available for output).\n",
      "\n",
      "  --ixtab   --oxtab   --xtab      Pretty-printed vertical-tabular.\n",
      "                      --xvright   Right-justifies values for XTAB format.\n",
      "\n",
      "  --ijson   --ojson   --json      JSON tabular: sequence or list of one-level\n",
      "                                  maps: {...}{...} or [{...},{...}].\n",
      "    --json-map-arrays-on-input    JSON arrays are unmillerable. --json-map-arrays-on-input\n",
      "    --json-skip-arrays-on-input   is the default: arrays are converted to integer-indexed\n",
      "    --json-fatal-arrays-on-input  maps. The other two options cause them to be skipped, or\n",
      "                                  to be treated as errors.  Please use the jq tool for full\n",
      "                                  JSON (pre)processing.\n",
      "                      --jvstack   Put one key-value pair per line for JSON\n",
      "                                  output.\n",
      "                      --jlistwrap Wrap JSON output in outermost [ ].\n",
      "                    --jknquoteint Do not quote non-string map keys in JSON output.\n",
      "                     --jvquoteall Quote map values in JSON output, even if they're\n",
      "                                  numeric.\n",
      "              --jflatsep {string} Separator for flattening multi-level JSON keys,\n",
      "                                  e.g. '{\"a\":{\"b\":3}}' becomes a:b => 3 for\n",
      "                                  non-JSON formats. Defaults to :.\n",
      "\n",
      "  -p is a keystroke-saver for --nidx --fs space --repifs\n",
      "\n",
      "  Examples: --csv for CSV-formatted input and output; --idkvp --opprint for\n",
      "  DKVP-formatted input and pretty-printed output.\n",
      "\n",
      "  Please use --iformat1 --oformat2 rather than --format1 --oformat2.\n",
      "  The latter sets up input and output flags for format1, not all of which\n",
      "  are overridden in all cases by setting output format to format2.\n",
      "\n",
      "Comments in data:\n",
      "  --skip-comments                 Ignore commented lines (prefixed by \"#\")\n",
      "                                  within the input.\n",
      "  --skip-comments-with {string}   Ignore commented lines within input, with\n",
      "                                  specified prefix.\n",
      "  --pass-comments                 Immediately print commented lines (prefixed by \"#\")\n",
      "                                  within the input.\n",
      "  --pass-comments-with {string}   Immediately print commented lines within input, with\n",
      "                                  specified prefix.\n",
      "Notes:\n",
      "* Comments are only honored at the start of a line.\n",
      "* In the absence of any of the above four options, comments are data like\n",
      "  any other text.\n",
      "* When pass-comments is used, comment lines are written to standard output\n",
      "  immediately upon being read; they are not part of the record stream.\n",
      "  Results may be counterintuitive. A suggestion is to place comments at the\n",
      "  start of data files.\n",
      "\n",
      "Format-conversion keystroke-saver options, for input, output, or both:\n",
      "As keystroke-savers for format-conversion you may use the following:\n",
      "        --c2t --c2d --c2n --c2j --c2x --c2p --c2m\n",
      "  --t2c       --t2d --t2n --t2j --t2x --t2p --t2m\n",
      "  --d2c --d2t       --d2n --d2j --d2x --d2p --d2m\n",
      "  --n2c --n2t --n2d       --n2j --n2x --n2p --n2m\n",
      "  --j2c --j2t --j2d --j2n       --j2x --j2p --j2m\n",
      "  --x2c --x2t --x2d --x2n --x2j       --x2p --x2m\n",
      "  --p2c --p2t --p2d --p2n --p2j --p2x       --p2m\n",
      "The letters c t d n j x p m refer to formats CSV, TSV, DKVP, NIDX, JSON, XTAB,\n",
      "PPRINT, and markdown, respectively. Note that markdown format is available for\n",
      "output only.\n",
      "\n",
      "Compressed-data options:\n",
      "  --prepipe {command} This allows Miller to handle compressed inputs. You can do\n",
      "  without this for single input files, e.g. \"gunzip < myfile.csv.gz | mlr ...\".\n",
      "  However, when multiple input files are present, between-file separations are\n",
      "  lost; also, the FILENAME variable doesn't iterate. Using --prepipe you can\n",
      "  specify an action to be taken on each input file. This pre-pipe command must\n",
      "  be able to read from standard input; it will be invoked with\n",
      "    {command} < {filename}.\n",
      "  Examples:\n",
      "    mlr --prepipe 'gunzip'\n",
      "    mlr --prepipe 'zcat -cf'\n",
      "    mlr --prepipe 'xz -cd'\n",
      "    mlr --prepipe cat\n",
      "  Note that this feature is quite general and is not limited to decompression\n",
      "  utilities. You can use it to apply per-file filters of your choice.\n",
      "  For output compression (or other) utilities, simply pipe the output:\n",
      "    mlr ... | {your compression command}\n",
      "\n",
      "Separator options, for input, output, or both:\n",
      "  --rs     --irs     --ors              Record separators, e.g. 'lf' or '\\r\\n'\n",
      "  --fs     --ifs     --ofs  --repifs    Field separators, e.g. comma\n",
      "  --ps     --ips     --ops              Pair separators, e.g. equals sign\n",
      "\n",
      "  Notes about line endings:\n",
      "  * Default line endings (--irs and --ors) are \"auto\" which means autodetect from\n",
      "    the input file format, as long as the input file(s) have lines ending in either\n",
      "    LF (also known as linefeed, '\\n', 0x0a, Unix-style) or CRLF (also known as\n",
      "    carriage-return/linefeed pairs, '\\r\\n', 0x0d 0x0a, Windows style).\n",
      "  * If both irs and ors are auto (which is the default) then LF input will lead to LF\n",
      "    output and CRLF input will lead to CRLF output, regardless of the platform you're\n",
      "    running on.\n",
      "  * The line-ending autodetector triggers on the first line ending detected in the input\n",
      "    stream. E.g. if you specify a CRLF-terminated file on the command line followed by an\n",
      "    LF-terminated file then autodetected line endings will be CRLF.\n",
      "  * If you use --ors {something else} with (default or explicitly specified) --irs auto\n",
      "    then line endings are autodetected on input and set to what you specify on output.\n",
      "  * If you use --irs {something else} with (default or explicitly specified) --ors auto\n",
      "    then the output line endings used are LF on Unix/Linux/BSD/MacOSX, and CRLF on Windows.\n",
      "\n",
      "  Notes about all other separators:\n",
      "  * IPS/OPS are only used for DKVP and XTAB formats, since only in these formats\n",
      "    do key-value pairs appear juxtaposed.\n",
      "  * IRS/ORS are ignored for XTAB format. Nominally IFS and OFS are newlines;\n",
      "    XTAB records are separated by two or more consecutive IFS/OFS -- i.e.\n",
      "    a blank line. Everything above about --irs/--ors/--rs auto becomes --ifs/--ofs/--fs\n",
      "    auto for XTAB format. (XTAB's default IFS/OFS are \"auto\".)\n",
      "  * OFS must be single-character for PPRINT format. This is because it is used\n",
      "    with repetition for alignment; multi-character separators would make\n",
      "    alignment impossible.\n",
      "  * OPS may be multi-character for XTAB format, in which case alignment is\n",
      "    disabled.\n",
      "  * TSV is simply CSV using tab as field separator (\"--fs tab\").\n",
      "  * FS/PS are ignored for markdown format; RS is used.\n",
      "  * All FS and PS options are ignored for JSON format, since they are not relevant\n",
      "    to the JSON format.\n",
      "  * You can specify separators in any of the following ways, shown by example:\n",
      "    - Type them out, quoting as necessary for shell escapes, e.g.\n",
      "      \"--fs '|' --ips :\"\n",
      "    - C-style escape sequences, e.g. \"--rs '\\r\\n' --fs '\\t'\".\n",
      "    - To avoid backslashing, you can use any of the following names:\n",
      "      cr crcr newline lf lflf crlf crlfcrlf tab space comma pipe slash colon semicolon equals\n",
      "  * Default separators by format:\n",
      "      File format  RS       FS       PS\n",
      "      gen          N/A      (N/A)    (N/A)\n",
      "      dkvp         auto     ,        =\n",
      "      json         auto     (N/A)    (N/A)\n",
      "      nidx         auto     space    (N/A)\n",
      "      csv          auto     ,        (N/A)\n",
      "      csvlite      auto     ,        (N/A)\n",
      "      markdown     auto     (N/A)    (N/A)\n",
      "      pprint       auto     space    (N/A)\n",
      "      xtab         (N/A)    auto     space\n",
      "\n",
      "Relevant to CSV/CSV-lite input only:\n",
      "  --implicit-csv-header Use 1,2,3,... as field labels, rather than from line 1\n",
      "                     of input files. Tip: combine with \"label\" to recreate\n",
      "                     missing headers.\n",
      "  --allow-ragged-csv-input|--ragged If a data line has fewer fields than the header line,\n",
      "                     fill remaining keys with empty string. If a data line has more\n",
      "                     fields than the header line, use integer field labels as in\n",
      "                     the implicit-header case.\n",
      "  --headerless-csv-output   Print only CSV data lines.\n",
      "  -N                 Keystroke-saver for --implicit-csv-header --headerless-csv-output.\n",
      "\n",
      "Double-quoting for CSV output:\n",
      "  --quote-all        Wrap all fields in double quotes\n",
      "  --quote-none       Do not wrap any fields in double quotes, even if they have\n",
      "                     OFS or ORS in them\n",
      "  --quote-minimal    Wrap fields in double quotes only if they have OFS or ORS\n",
      "                     in them (default)\n",
      "  --quote-numeric    Wrap fields in double quotes only if they have numbers\n",
      "                     in them\n",
      "  --quote-original   Wrap fields in double quotes if and only if they were\n",
      "                     quoted on input. This isn't sticky for computed fields:\n",
      "                     e.g. if fields a and b were quoted on input and you do\n",
      "                     \"put '$c = $a . $b'\" then field c won't inherit a or b's\n",
      "                     was-quoted-on-input flag.\n",
      "\n",
      "Numerical formatting:\n",
      "  --ofmt {format}    E.g. %.18lf, %.0lf. Please use sprintf-style codes for\n",
      "                     double-precision. Applies to verbs which compute new\n",
      "                     values, e.g. put, stats1, stats2. See also the fmtnum\n",
      "                     function within mlr put (mlr --help-all-functions).\n",
      "                     Defaults to %lf.\n",
      "\n",
      "Other options:\n",
      "  --seed {n} with n of the form 12345678 or 0xcafefeed. For put/filter\n",
      "                     urand()/urandint()/urand32().\n",
      "  --nr-progress-mod {m}, with m a positive integer: print filename and record\n",
      "                     count to stderr every m input records.\n",
      "  --from {filename}  Use this to specify an input file before the verb(s),\n",
      "                     rather than after. May be used more than once. Example:\n",
      "                     \"mlr --from a.dat --from b.dat cat\" is the same as\n",
      "                     \"mlr cat a.dat b.dat\".\n",
      "  -n                 Process no input files, nor standard input either. Useful\n",
      "                     for mlr put with begin/end statements only. (Same as --from\n",
      "                     /dev/null.) Also useful in \"mlr -n put -v '...'\" for\n",
      "                     analyzing abstract syntax trees (if that's your thing).\n",
      "  -I                 Process files in-place. For each file name on the command\n",
      "                     line, output is written to a temp file in the same\n",
      "                     directory, which is then renamed over the original. Each\n",
      "                     file is processed in isolation: if the output format is\n",
      "                     CSV, CSV headers will be present in each output file;\n",
      "                     statistics are only over each file's own records; and so on.\n",
      "\n",
      "Then-chaining:\n",
      "Output of one verb may be chained as input to another using \"then\", e.g.\n",
      "  mlr stats1 -a min,mean,max -f flag,u,v -g color then sort -f color\n",
      "\n",
      "Auxiliary commands:\n",
      "Miller has a few otherwise-standalone executables packaged within it.\n",
      "They do not participate in any other parts of Miller.\n",
      "Available subcommands:\n",
      "  aux-list\n",
      "  lecat\n",
      "  termcvt\n",
      "  hex\n",
      "  unhex\n",
      "  netbsd-strptime\n",
      "For more information, please invoke mlr {subcommand} --help\n",
      "\n",
      "For more information please see http://johnkerl.org/miller/doc and/or\n",
      "http://github.com/johnkerl/miller. This is Miller version 5.7.0.\n"
     ]
    }
   ],
   "source": [
    "!mlr --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
